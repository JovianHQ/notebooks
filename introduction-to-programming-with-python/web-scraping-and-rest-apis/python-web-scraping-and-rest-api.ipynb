{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "determined-chosen",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping and REST APIs \n",
    "\n",
    "This tutorial is a part of the [Zero to Data Analyst Bootcamp by Jovian](https://www.jovian.ai/data-analyst-bootcamp)\n",
    "\n",
    "![](https://i.imgur.com/6zM7JBq.png)\n",
    "\n",
    "\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning. While web scraping often involves parsing and processing [HTML documents](https://developer.mozilla.org/en-US/docs/Web/HTML), some platforms offer REST APIs to retrieve information in a machine-readable format like JSON. In this tutorial, we'll use web scraping and REST APIs to create a real-world dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-shanghai",
   "metadata": {},
   "source": [
    "The following topics are covered in this tutorial:\n",
    "\n",
    "* Downloading web pages using the requests library\n",
    "* Inspecting the HTML source code of a web page\n",
    "* Parsing parts of a website using Beautiful Soup\n",
    "* Writing parsed information into CSV files\n",
    "* Using a REST API to retrieve data as JSON\n",
    "* Combining data from multiple sources\n",
    "* Using links on a page to crawl a website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-strand",
   "metadata": {},
   "source": [
    "### How to Run the Code\n",
    "\n",
    "The best way to learn the material is to execute the code and experiment with it yourself. This tutorial is an executable [Jupyter notebook](https://jupyter.org). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
    "\n",
    "#### Option 1: Running using free online resources (1-click, recommended)\n",
    "\n",
    "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Binder**. You can also select \"Run on Colab\" or \"Run on Kaggle\", but you'll need to create an account on [Google Colab](https://colab.research.google.com) or [Kaggle](https://kaggle.com) to use these platforms.\n",
    "\n",
    "\n",
    "#### Option 2: Running on your computer locally\n",
    "\n",
    "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-recipient",
   "metadata": {},
   "source": [
    "## Problem \n",
    "\n",
    "Over the course of this tutorial, we'll solve the following problem to learn the tools and techniques used for web scraping:\n",
    "\n",
    "\n",
    "> **QUESTION**: Write a Python function that creates a CSV file (comma-separated values) containing details about the 25 top GitHub repositories for any given topic. The top repositories for the topic `machine-learning` can be found on this page: [https://github.com/topics/machine-learning](https://github.com/topics/machine-learning). The output CSV should contain these details: repository name, owner's username, no. of stars, repository URL. \n",
    "\n",
    "\n",
    " <a href=\"https://github.com/topics/machine-learning\"><img src=\"https://i.imgur.com/5V1HGLs.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\"></a>\n",
    " \n",
    " \n",
    "How would you go about solving this problem in Python? Explore the web page and take a couple of minutes to come up with an approach before proceeding further. How many lines of code do you think the solution will require?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-forth",
   "metadata": {},
   "source": [
    "## Downloading a web page using `requests`\n",
    "\n",
    "When you access a URL like https://github.com/topics/machine-learning using a web browser, it downloads the contents of the web page the URL points to and displays them on the screen. Before we can extract information from a web page using Python, we need to download the page.\n",
    "\n",
    "We'll use a library called [`requests`](https://docs.python-requests.org/en/master/) to download web pages from the internet. Let's begin by installing and importing the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "domestic-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optional-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-tribute",
   "metadata": {},
   "source": [
    "We get download a web page using the `requests.get` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fallen-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url = 'https://github.com/topics/machine-learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "looking-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tough-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-dover",
   "metadata": {},
   "source": [
    "`requests.get` returns a response object, which contains the contents of the web page and some information indicating whether the request was successful, using a status code. Learn more about HTTP status codes here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status. \n",
    "\n",
    " If the request was successful, `response.status_code` is set to a value between 200 and 299. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "personal-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-cattle",
   "metadata": {},
   "source": [
    "The contents of the web page can be accessed using the `.text` property of the `response`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lined-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-mention",
   "metadata": {},
   "source": [
    "Let's view the first 1000 characters of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eleven-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-uGiH6wbEDXS0vWuvN3hZbENUuT1jRMWy2XVfJIgd3mEESUBtD/hnFdIiujVyRcPJ5dofwZ6e196xmCczSkgz9g==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-b86887eb06c40d74b4bd6baf3778596c.css\" />\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-gEUpuli94xYShC0AAbAVQoQqxAoVyNDUWuD3x6Hsvwm8f1L7gbiu4bEM1HDLEkRz4ofHAvdAdmeqaUtzBCy6xg==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/site-804529ba58bde31612842d0001b01542.css\" />\\n    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-8rXKu7ZOFdS3H7Rk0wJ38WQFoEp6b7HTSZ58yDoWzKX+'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-screen",
   "metadata": {},
   "source": [
    "What you're seeing above is the *source code* of the web page, written in a language called [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML). It defines the content and structure of the web page. \n",
    "\n",
    "Let's save the contents to a file with the `.html` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "swedish-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('machine-learning-topics.html', 'w') as file:\n",
    "    file.write(page_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-spirit",
   "metadata": {},
   "source": [
    "You can now view the file using the \"File > Open\" menu option within Jupyter and clicking on *machine-learning.html* in the list of files displayed. Here's what you'll see when you open the file:\n",
    "\n",
    "<img src=\"https://i.imgur.com/8gEbT1P.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "While this looks similar to the original web page, note that it's simply a copy. You will notice that none of the links or buttons actually work. To view or edit the source code of the file, click \"File > Open\" within Jupyter, then select the file *machine-learning.html* from the list and click the \"Edit\" button.\n",
    "\n",
    "<img src=\"https://i.imgur.com/JG7Q8CK.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "As you might expect, the source code looks something like this:\n",
    "\n",
    "<img src=\"https://i.imgur.com/6ynXNdz.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "Try scrolling through the source code. Can you make sense of it? Can you see how the information on the page is organized within the file? We'll learn more about it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-wilderness",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Download the web page for a different topic e.g. https://github.com/topics/data-analysis using `requests` and save it to a file e.g. `data-analysis.html`. View the page and compare it with the previously downloaded page? How are the two different? Can you spot the differences in the source code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-march",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-transcript",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-maine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fallen-sterling",
   "metadata": {},
   "source": [
    "Let's save our work using `jovian` before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "virgin-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "laden-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grateful-atlantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project='python-web-scraping-and-rest-api')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-setup",
   "metadata": {},
   "source": [
    "## Inspecting the HTML source code of a web page\n",
    "\n",
    "![](https://i.imgur.com/mvBpQIP.png)\n",
    "\n",
    "As mentioned earlier, web pages are written in a language called HTML (Hyper Text Markup Language). HTML is a fairly simple language comprised of *tags*  (also called *nodes* or *elements*) e.g. `<a href=\"https://jovian.ai\" target=\"_blank\">Go to Jovian</a>`. An HTML tag has three parts:\n",
    "\n",
    "1. **Name**: E.g. `html`, `head`, `body`, `div` etc. The name indicates what the tag represents and how a browser should interpret the information inside it.\n",
    "2. **Attributes**: E.g. `href`, `target`, `class`, `id` etc. These are used set properties for a tag and are used by the browser to customize how a tag is displayed and what happens when a user interacts with it.\n",
    "3. **Children**: A tag can contain some text or other tags or both between the opening and closing segments e.g. `<div>Some content</div>`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-bristol",
   "metadata": {},
   "source": [
    "### Inside an HTML Document\n",
    "\n",
    "Here's a simple HTML document that uses many commonly used tags:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>All About Python</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div style=\"width: 640px; margin: 40px auto\">\n",
    "      <h1 style=\"text-align:center;\">Python - A Programming Language</h1>\n",
    "      <img src=\"https://www.python.org/static/community_logos/python-logo-master-v3-TM.png\" alt=\"python-logo\" style=\"width:240px;margin:0 auto;display:block;\">\n",
    "      <div>\n",
    "        <h2>About Python</h2>\n",
    "        <p>\n",
    "          Python is an <span style=\"font-style: italic\">interpreted, high-level and general-purpose</span> programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Visit the <a href=\"https://docs.python.org/3/\">official documentation</a> to learn more.\n",
    "        </p>\n",
    "      </div>\n",
    "      <div>\n",
    "        <h2>Some Python Libraries</h2>\n",
    "        <ul id=\"libraries\">\n",
    "          <li>Numpy</li>\n",
    "          <li>Pandas</li>\n",
    "          <li>PyTorch</li>\n",
    "          <li>Scikit Learn</li>\n",
    "        </ul>\n",
    "      </div>\n",
    "      <div>\n",
    "        <h2>Recent Python Versions</h2>\n",
    "        <table id=\"versions-table\">\n",
    "          <tr>\n",
    "            <th class=\"bordered-table\">Version</th>\n",
    "            <th class=\"bordered-table\">Released on</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td class=\"bordered-table\">Python 3.8</td>\n",
    "            <td class=\"bordered-table\">October 2019</td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "            <td class=\"bordered-table\">Python 3.7</td>\n",
    "            <td class=\"bordered-table\">June 2018</td>\n",
    "          </tr>\n",
    "        </table>\n",
    "          <style>\n",
    "              .bordered-table { \n",
    "                  border: 1px solid black; padding: 8px;\n",
    "              }\n",
    "          </style>\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "```\n",
    "\n",
    "> **EXERCISE**: Copy the above HTML code and paste into a new file called `webpage.html`. To create a new file, first select \"File > Open\" from the menu bar, then select \"New > Text\" file. View the saved file. Can you see how the different tags are displayed in different ways by the browser?\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/lcSHz5V.png\" width=\"480\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "> **EXERCISE**: Make some changes to the code inside `webpage.html`. Save the file and view it again. Do you see your changes reflected? Play with the structure of the file. Try to break things and fix them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-election",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-scope",
   "metadata": {},
   "source": [
    "### Common Tags and Attributes\n",
    "\n",
    "Following are some of the most commonly used HTML tags:\n",
    "\n",
    "* `html`\n",
    "* `head`\n",
    "* `title`\n",
    "* `body`\n",
    "* `div`\n",
    "* `span`\n",
    "* `h1` to `h6`\n",
    "* `p`\n",
    "* `img`\n",
    "* `ul`, `ol` and `li`\n",
    "* `table`, `tr`, `th` and `td`\n",
    "* `style`\n",
    "* ...\n",
    "\n",
    "Each tags supports several attributes. Following are some common attributes used to modify the behavior of tags:\n",
    "\n",
    "* `id`\n",
    "* `style`\n",
    "* `class`\n",
    "* `href` (used with `<a>`)\n",
    "* `src` (used with `<img>`)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> **EXERCISE**: Complete this tutorial on HTML: https://www.htmldog.com/guides/html/ . Once done, describe what the above tags and attributes are used for. Try creating a new HTML page using the tags you find most interesting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "charitable-sherman",
   "metadata": {},
   "source": [
    "### Inspecting HTML in the Browser\n",
    "\n",
    "You can view the source code of any webpage right within your browser by right clicking anywhere on a page and selecting the \"Inspect\" option. It opens the \"Developer Tools\" pane where you can see the source code as a tree. You can expand and collapse various nodes and find the source code for a specific portion of the page.\n",
    "\n",
    "Here's what it looks like on the Chrome browser:\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/jCA1T6Z.png\" width=\"640\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "\n",
    "> **EXERCISE**: Explore the source code of the web page https://github.com/topics/machine-learning . Try to find the portions in the source code corresponding to the username, repostiory name and number of stars for each repository listed on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-breach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tribal-recruitment",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interstate-seeker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-organization",
   "metadata": {},
   "source": [
    "## Extracting information from HTML using Beautiful Soup\n",
    "\n",
    "To extract information the HTML source code of a webpage programmatically, we can use the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) library. Let's install the library and import the `BeautifulSoup` class from `bs4` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exclusive-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "competitive-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "uniform-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "?BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-parameter",
   "metadata": {},
   "source": [
    "Next, let's read the contents of the file `machine-learning.html` and create a `BeautifulSoup` object to parse the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indonesian-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('machine-learning.html', 'r') as f:\n",
    "    html_source = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sapphire-decline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-uGiH6wbEDXS0vWuvN3hZbENUuT1jRMWy2XVfJIgd3mEESUBtD/hnFdIiujVyRcPJ5dofwZ6e196xmCczSkgz9g==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-b86887eb06c40d74b4bd6baf3778596c.css\" />\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-gEUpuli94xYShC0AAbAVQoQqxAoVyNDUWuD3x6Hsvwm8f1L7gbiu4bEM1HDLEkRz4ofHAvdAdmeqaUtzBCy6xg==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/site-804529ba58bde31612842d0001b01542.css\" />\\n    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-8rXKu7ZOFdS3H7Rk0wJ38WQFoEp6b7HTSZ58yDoWzKX+'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_source[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "commercial-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(html_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-annex",
   "metadata": {},
   "source": [
    "The `doc` object contains several useful properties and methods to extract information from the HTML document. Let's look at a few examples below.\n",
    "\n",
    "**NOTE**: You don't need to remember all (or any) of the properties/methods, you can look up [the documentation of BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) or [just search online](https://www.google.co.in/search?q=beautifulsoup+how+to+get+href+of+link) to find exactly what you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-partnership",
   "metadata": {},
   "source": [
    "### Accessing a tag\n",
    "\n",
    "> **QUESTION**: Find the title of the page represented by `doc`.\n",
    "\n",
    "The title of the page is contained within the `<title>` tag. We can access the title tag using `doc.title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "patient-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tag = doc.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "developed-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>machine-learning · GitHub Topics · GitHub</title>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tropical-prompt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(title_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-provision",
   "metadata": {},
   "source": [
    "We can access a tag's name using the `.name` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "disturbed-herald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-portuguese",
   "metadata": {},
   "source": [
    "The text within a tag can be accessed using `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cooked-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine-learning · GitHub Topics · GitHub'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tag.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-prairie",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Explore the `html`, `body` and `head` tags of `doc`. Do you see what you expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-heart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "patient-career",
   "metadata": {},
   "source": [
    "If a tag occurs more than once in a document e.g. `<a>` (which represents links), then `doc.a` finds the first `<a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "curious-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = doc.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "advisory-occupation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "expressed-going",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to content'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-browser",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the first occurrence of each of these tags in `doc`: `div`, `img`, `span`, `p` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-nomination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pleased-egyptian",
   "metadata": {},
   "source": [
    "### Finding all tags of the same type\n",
    "\n",
    "To find all the occurrence of a tag, use the `find_all` method.\n",
    "\n",
    "> **QUESTION**: Find all the link tags on the page. How many links does the page contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "electoral-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link_tags = doc.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "protected-blast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_link_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "labeled-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>,\n",
       " <a aria-label=\"Homepage\" class=\"mr-4\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\" href=\"https://github.com/\">\n",
       " <svg aria-hidden=\"true\" class=\"octicon octicon-mark-github color-text-white\" height=\"32\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"32\"><path d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z\" fill-rule=\"evenodd\"></path></svg>\n",
       " </a>,\n",
       " <a class=\"d-inline-block d-lg-none f5 color-text-white no-underline border color-border-tertiary rounded-2 px-2 py-1 mr-3 mr-sm-5 js-signup-redesign-control js-signup-redesign-target\" data-hydro-click='{\"event_type\":\"authentication.click\",\"payload\":{\"location_in_page\":\"site header\",\"repository_id\":null,\"auth_type\":\"SIGN_UP\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"e755e218b5b5c8ea0dcf7c1afea104ce5b0e7bb40929d477942f46cf5b206533\" href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "                 Sign up\n",
       "               </a>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_link_tags[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-comment",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Get a list of all the `img` tags on the page. How many images does the page contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-postcard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "smoking-chambers",
   "metadata": {},
   "source": [
    "### Accessing attributes\n",
    "\n",
    "The attributes of a tag can be accessed using the indexing notation e.g. `first_link['href']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "relevant-classics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "appreciated-colleague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#start-of-content'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "opening-granny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['px-2',\n",
       " 'py-4',\n",
       " 'color-bg-info-inverse',\n",
       " 'color-text-white',\n",
       " 'show-on-focus',\n",
       " 'js-skip-to-content']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-exchange",
   "metadata": {},
   "source": [
    "Note that the `class` attribute is automatically split into a list of classes (this isn't done for any other attribute). This is because it's common practice to check for a specific class within a tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-minnesota",
   "metadata": {},
   "source": [
    "You can use the `.attrs` property to view all the attributes as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "electoral-editor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '#start-of-content',\n",
       " 'class': ['px-2',\n",
       "  'py-4',\n",
       "  'color-bg-info-inverse',\n",
       "  'color-text-white',\n",
       "  'show-on-focus',\n",
       "  'js-skip-to-content']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-rwanda",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the 5th image tag on the page (counting from 0). Which attributes does the tag contain? Find the values of the `src` and `alt` attributes of the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-phone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sonic-occupation",
   "metadata": {},
   "source": [
    "### Searching by Attribute Value\n",
    "\n",
    "> **QUESTION**: Find the `img` tag(s) on the page with the `alt` attribute set to `tsbertalan`.\n",
    "\n",
    "We can provide a dictionary of attribute as the second argument to `find_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ordered-blond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"tsbertalan\" class=\"avatar avatar-user avatar-small\" height=\"32\" src=\"https://avatars.githubusercontent.com/u/306137?v=4\" width=\"32\"/>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find_all('img', { 'alt': 'tsbertalan'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-infection",
   "metadata": {},
   "source": [
    "If we're just interested in the first element, we can use the `find` method. Keep in mind that `find` returns `None` if no matching tag was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acoustic-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img alt=\"tsbertalan\" class=\"avatar avatar-user avatar-small\" height=\"32\" src=\"https://avatars.githubusercontent.com/u/306137?v=4\" width=\"32\"/>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('img', { 'alt': 'tsbertalan'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-adelaide",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the `src` attribute of the first `img` tag with the `alt` attribute set to `julia`. Visit the link and check what the image represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-hollywood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "involved-statistics",
   "metadata": {},
   "source": [
    "### Searching by Class\n",
    "\n",
    "The `class` attribute is one of the most frequently used attributes on HTML tags (because it is used for layout and styling). We can search for tags containing a class using the `class_` argument in `find_all` (note that `class` is a reserved keyword in Python, hence the underscore in the argument name).\n",
    "\n",
    "> **QUESTION**: Find all the tags containing the class `HeaderMenu-link`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "superb-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_tags = doc.find_all(class_='HeaderMenu-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "congressional-neutral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
       "                     Why GitHub?\n",
       "                     <svg class=\"icon-chevon-down-mktg position-absolute position-lg-relative\" fill=\"none\" viewbox=\"0 0 14 8\" x=\"0px\" xml:space=\"preserve\" y=\"0px\">\n",
       " <path d=\"M1,1l6.2,6L13,1\"></path>\n",
       " </svg>\n",
       " </summary>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Team\" href=\"/team\">Team</a>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Enterprise\" href=\"/enterprise\">Enterprise</a>,\n",
       " <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
       "                     Explore\n",
       "                     <svg class=\"icon-chevon-down-mktg position-absolute position-lg-relative\" fill=\"none\" viewbox=\"0 0 14 8\" x=\"0px\" xml:space=\"preserve\" y=\"0px\">\n",
       " <path d=\"M1,1l6.2,6L13,1\"></path>\n",
       " </svg>\n",
       " </summary>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Marketplace\" href=\"/marketplace\">Marketplace</a>,\n",
       " <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
       "                     Pricing\n",
       "                     <svg class=\"icon-chevon-down-mktg position-absolute position-lg-relative\" fill=\"none\" viewbox=\"0 0 14 8\" x=\"0px\" xml:space=\"preserve\" y=\"0px\">\n",
       " <path d=\"M1,1l6.2,6L13,1\"></path>\n",
       " </svg>\n",
       " </summary>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 no-underline mr-3\" data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\" data-hydro-click='{\"event_type\":\"authentication.click\",\"payload\":{\"location_in_page\":\"site header menu\",\"repository_id\":null,\"auth_type\":\"SIGN_UP\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"77fa0805c2ee5e083e5dbe5571a2e37d8661eca3a115806e97d54914b8332ea9\" href=\"/login?return_to=%2Ftopics%2Fmachine-learning\">\n",
       "           Sign in\n",
       "         </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded px-2 py-1 js-signup-redesign-target js-signup-redesign-control\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded-1 px-2 py-1 js-signup-redesign-target js-signup-redesign-variation\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" hidden=\"\" href=\"/join_next?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-stations",
   "metadata": {},
   "source": [
    "We can also for a specific type of tag e.g. `<a>` matching the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "detailed-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_link_tags = doc.find_all('a', class_='HeaderMenu-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "careful-celebration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Team\" href=\"/team\">Team</a>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Enterprise\" href=\"/enterprise\">Enterprise</a>,\n",
       " <a class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Marketplace\" href=\"/marketplace\">Marketplace</a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 no-underline mr-3\" data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\" data-hydro-click='{\"event_type\":\"authentication.click\",\"payload\":{\"location_in_page\":\"site header menu\",\"repository_id\":null,\"auth_type\":\"SIGN_UP\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"77fa0805c2ee5e083e5dbe5571a2e37d8661eca3a115806e97d54914b8332ea9\" href=\"/login?return_to=%2Ftopics%2Fmachine-learning\">\n",
       "           Sign in\n",
       "         </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded px-2 py-1 js-signup-redesign-target js-signup-redesign-control\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>,\n",
       " <a class=\"HeaderMenu-link flex-shrink-0 d-inline-block no-underline border color-border-tertiary rounded-1 px-2 py-1 js-signup-redesign-target js-signup-redesign-variation\" data-hydro-click='{\"event_type\":\"analytics.click\",\"payload\":{\"category\":\"Sign up\",\"action\":\"click to sign up for account\",\"label\":\"ref_page:/topics/machine-learning;ref_cta:Sign up;ref_loc:header logged out\",\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"0494fadbb426567cd99a66c59ec732aa2e99e4e75a2992dbbbad091df5f6b39d\" hidden=\"\" href=\"/join_next?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2Ftopics%2Fmachine-learning&amp;source=header\">\n",
       "               Sign up\n",
       "             </a>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_link_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-accreditation",
   "metadata": {},
   "source": [
    "### Parsing Information from Tags\n",
    "\n",
    "Once we have a list of tags matching a given criteria, it's easy to extract information and convert it to a more convenient format.\n",
    "\n",
    "> **QUESTION**: Find the link text and URL of all the links with the header of page contained in `doc`.\n",
    "\n",
    "We'll create a list of dictionaries containing the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ranking-parade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Team', 'url': 'https://github.com/team'},\n",
       " {'title': 'Enterprise', 'url': 'https://github.com/enterprise'},\n",
       " {'title': 'Marketplace', 'url': 'https://github.com/marketplace'},\n",
       " {'title': 'Sign in',\n",
       "  'url': 'https://github.com/login?return_to=%2Ftopics%2Fmachine-learning'},\n",
       " {'title': 'Sign up',\n",
       "  'url': 'https://github.com/join?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2Ftopics%2Fmachine-learning&source=header'},\n",
       " {'title': 'Sign up',\n",
       "  'url': 'https://github.com/join_next?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2Ftopics%2Fmachine-learning&source=header'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_links = []\n",
    "base_url = 'https://github.com'\n",
    "\n",
    "for tag in header_link_tags:\n",
    "    header_links.append({ 'title': tag.text.strip(), 'url': base_url + tag['href']})\n",
    "    \n",
    "header_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-glasgow",
   "metadata": {},
   "source": [
    "Thus, we have successfully extracted some links form the page. This is exactly what scraping is: downloading a webpage, parsing the HTML and extracting some useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-election",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Find the list of all the images matching the class `avatar-user`. Each element of the list should be a dictionary containing two keys, `\"username\"` and `\"url\"`. You can obtain the username using the `alt` attribute of a tag and the URL using the `src` attribute of a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-enhancement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-revolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "upset-glory",
   "metadata": {},
   "source": [
    "### Elements inside a tag\n",
    "\n",
    "> **QUESTION**: Find the tags contained within the `ul` tag in the sample HTML document below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "meaningful-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <ul>\n",
    "            <li>Item 1</li>\n",
    "            <li>Item 2</li>\n",
    "            <li>Item 3</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "racial-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = BeautifulSoup(sample_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "revised-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tag = sample_doc.find('ul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-makeup",
   "metadata": {},
   "source": [
    "We can use the `find_all` method on the tag, and set `recursive=False` to find just the direct children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "located-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_item_tags = list_tag.find_all('li', recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "governing-table",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li>Item 1</li>, <li>Item 2</li>, <li>Item 3</li>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_item_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-westminster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-decimal",
   "metadata": {},
   "source": [
    "Keep in mind that you don't need to remember all (or any) of the methods or properties offered by Beautiful Soup documents and tags. You look up the documentation or simply Google what you're trying to do or ask a question on StackOverflow.\n",
    "\n",
    "You should be able to figure out what you need to do, when you need to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-development",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "accepted-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-contrast",
   "metadata": {},
   "source": [
    "### Top Repositories for a Topic\n",
    "\n",
    "Let's return to our original problem statement of finding the top repositories for a given topic. Before we parse a page and find the top repositories let's define a helper function to get the contents of the web page for any topic.\n",
    "\n",
    "> **QUESTION**: Define a function `get_topic_page` which downloads the GitHub web page for a given topic and returns a beautiful soup document representing the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "raising-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic):\n",
    "    # Construct the URL\n",
    "    topic_repos_url = 'https://github.com/topics/' + topic\n",
    "    \n",
    "    # Get the HTML page content using requests\n",
    "    response = requests.get(topic_repos_url)\n",
    "    \n",
    "    # Ensure that the reponse is valid\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + topic_repos_url)\n",
    "    \n",
    "    # Construct a beautiful soup document\n",
    "    doc = BeautifulSoup(response.text)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "alien-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('machine-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "through-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine-learning · GitHub Topics · GitHub'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-surgeon",
   "metadata": {},
   "source": [
    "Getting the topic page for another topic is now as simple as invoking the function with a different argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "medium-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = get_topic_page('data-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "documented-shuttle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data-analysis · GitHub Topics · GitHub'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-platinum",
   "metadata": {},
   "source": [
    "> **QUESTION**: Come with a strategy to find the repository name, owner's username, no. of stars and repository link for the repositories listed on a topic page.\n",
    "\n",
    "<img src=\"https://i.imgur.com/szL76cU.png\" width=\"640\" style=\"box-shadow:rgba(52, 64, 77, 0.2) 0px 1px 5px 0px;border-radius:4px;\">\n",
    "\n",
    "Upon inspecting the box containing the information for a repository, you will find that each repository is represented by an `article` tag with class attribute `border rounded color-shadow-small color-bg-secondary my-4`.\n",
    "\n",
    "Let's find all the `article` tags matching this class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "laden-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tags = doc.find_all('article', class_='border rounded color-shadow-small color-bg-secondary my-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "returning-moisture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-barrier",
   "metadata": {},
   "source": [
    "There are 30 repositories listed on the page, and our query resulted in 30 article tags. Looks like we've found the enclosing tag for each repository. We can verify this by looking inside one of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "romantic-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tag = article_tags[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "arctic-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to view\n",
    "# article_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-might",
   "metadata": {},
   "source": [
    "We need to extract the following information from each tag:\n",
    "\n",
    "1. Repository name\n",
    "2. Owner's username\n",
    "3. Number of stars\n",
    "4. Repository link\n",
    "\n",
    "If you look at the source of an article tag, you will notice that the repository name, owner's username and the repository link are all part of an `h1` tag.\n",
    "\n",
    "Let's retrieve the first `h1` inside an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "quiet-default",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"f3 color-text-secondary text-normal lh-condensed\">\n",
       "<a data-ga-click=\"Explore, go to repository owner, location:explore feed\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"REPOSITORY_CARD\",\"click_target\":\"OWNER\",\"click_visual_representation\":\"REPOSITORY_OWNER_HEADING\",\"actor_id\":null,\"record_id\":10386605,\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"1bae99446056a085802d3401cc23faad491e91eb64e36a9fbba9ac02ba2bb434\" href=\"/aymericdamien\">\n",
       "            aymericdamien\n",
       "</a>          /\n",
       "          <a class=\"text-bold\" data-ga-click=\"Explore, go to repository, location:explore feed\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"REPOSITORY_CARD\",\"click_target\":\"REPOSITORY\",\"click_visual_representation\":\"REPOSITORY_NAME_HEADING\",\"actor_id\":null,\"record_id\":45986162,\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"fef3fe4409b1dde6f30a7dba8f66011bd9736148681961664451736c9328262d\" href=\"/aymericdamien/TensorFlow-Examples\">\n",
       "            TensorFlow-Examples\n",
       "</a> </h1>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_tag = article_tag.find('h1')\n",
    "h1_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-swing",
   "metadata": {},
   "source": [
    "The `h1` has `a` tags inside it, one containing the owner's username and the second containing the repository title. The `href` of the second tag also contains the relative path of the repository. Let's extract this information from the `a` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "rotary-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = h1_tag.find_all('a', recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "convinced-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            aymericdamien\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = a_tags[0].text\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-moldova",
   "metadata": {},
   "source": [
    "Looks like the username contains some leading and trailing whitespace. We can get rid of it using `strip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "detailed-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aymericdamien'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = a_tags[0].text.strip()\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-corpus",
   "metadata": {},
   "source": [
    "We can get the repository name and repository path in the same fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "olive-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TensorFlow-Examples'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = a_tags[1].text.strip()\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "extensive-lobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aymericdamien/TensorFlow-Examples'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = a_tags[1]['href'].strip()\n",
    "repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-quality",
   "metadata": {},
   "source": [
    "To get the full URL to the repository, we can append the base URL `https://github.com` at the beginning of the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "welcome-vatican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/aymericdamien/TensorFlow-Examples'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://github.com'\n",
    "repo_url = base_url + repo_path \n",
    "repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-platinum",
   "metadata": {},
   "source": [
    "\n",
    "Next, to get the number of starts, we notice that it is contained within an `a` tag which has the count `social-count float-none`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fresh-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_star_tag = article_tags[4].find('a', class_='social-count float-none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "instant-musician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"social-count float-none\" data-ga-click=\"Explore, go to repository stargazers, location:explore feed\" data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"REPOSITORY_CARD\",\"click_target\":\"STARGAZERS\",\"click_visual_representation\":\"STARGAZERS_NUMBER\",\"actor_id\":null,\"record_id\":45986162,\"originating_url\":\"https://github.com/topics/machine-learning\",\"user_id\":null}}' data-hydro-click-hmac=\"a46079c0b03997f1f8a4f276ba3457dbf1123db76023ef8d1a4d701ec64b17da\" href=\"/aymericdamien/TensorFlow-Examples/stargazers\">\n",
       "          40.3k\n",
       "</a>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_star_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-deviation",
   "metadata": {},
   "source": [
    "Let's extract the star count from the `a` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "flexible-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40.3k'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_star_tag.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-termination",
   "metadata": {},
   "source": [
    "The `k` at the end indicates `1000`. Let's write a helper function which can convert strings like `40.3k` into the number `40,300`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "wireless-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    else:\n",
    "        return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cooked-florence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40300"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_star_count('40.3k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "identical-charlotte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_star_count('991')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-stations",
   "metadata": {},
   "source": [
    "We can now determine the star count as a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "selected-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_count = parse_star_count(a_star_tag.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "rotary-reggae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40300"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-divide",
   "metadata": {},
   "source": [
    "Perfect, we've extracted all the information we were interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "instant-spectacular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository name: TensorFlow-Examples\n",
      "Owner's username: aymericdamien\n",
      "Stars: 40300\n",
      "Repository URL: https://github.com/aymericdamien/TensorFlow-Examples\n"
     ]
    }
   ],
   "source": [
    "print('Repository name:', repo_name)\n",
    "print(\"Owner's username:\", username)\n",
    "print('Stars:', star_count)\n",
    "print('Repository URL:', repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-hours",
   "metadata": {},
   "source": [
    "Let's extract the logic for parsing the required information from an article tag into a function.\n",
    "\n",
    "> **QUESTION**: Write a function `parse_repostory` which returns a dictionary containing the repository name, owner's username, number of stars and repository URL by parsing a given `article` tag representing a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "third-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_repository(article_tag):\n",
    "    # <a> tags containing username, repository name and URL\n",
    "    a_tags = article_tag.h1.find_all('a')\n",
    "    # Owner's username\n",
    "    username = a_tags[0].text.strip()\n",
    "    # Repository name\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    # Repository URL\n",
    "    repo_url = base_url + a_tags[1]['href'].strip()\n",
    "    # Star count\n",
    "    stars_tag = article_tag.find('a', class_='social-count float-none')\n",
    "    star_count = parse_star_count(stars_tag.text.strip())\n",
    "    # Return a dictionary\n",
    "    return {\n",
    "        'repository_name': repo_name,\n",
    "        'owner_username': username,        \n",
    "        'stars': star_count,\n",
    "        'repository_url': repo_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-belarus",
   "metadata": {},
   "source": [
    "We can now use the function to parse any `article` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "differential-replacement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'tensorflow',\n",
       " 'owner_username': 'tensorflow',\n",
       " 'stars': 155000,\n",
       " 'repository_url': 'https://github.com/tensorflow/tensorflow'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_repository(article_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "regulated-endorsement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'caffe',\n",
       " 'owner_username': 'BVLC',\n",
       " 'stars': 31500,\n",
       " 'repository_url': 'https://github.com/BVLC/caffe'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_repository(article_tags[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-coordinator",
   "metadata": {},
   "source": [
    "We can even use a list comprehension to parse all the `article` tags in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "defensive-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_repositories = [parse_repository(tag) for tag in article_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "great-bench",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_repositories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "innovative-brooks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47300,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'},\n",
       " {'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45100,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'repository_name': 'TensorFlow-Examples',\n",
       "  'owner_username': 'aymericdamien',\n",
       "  'stars': 40300,\n",
       "  'repository_url': 'https://github.com/aymericdamien/TensorFlow-Examples'}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_repositories[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-insured",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **QUESTION**: Write a function that takes a `BeautifulSoup` object representing a topic page and returns a list of dictionaries containing information about the top repositories for the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "instant-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_repositories(doc):\n",
    "    article_tags = doc.find_all('article', class_='border rounded color-shadow-small color-bg-secondary my-4')\n",
    "    topic_repos = [parse_repository(tag) for tag in article_tags]\n",
    "    return topic_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-premium",
   "metadata": {},
   "source": [
    "We can now use the functions we've defined to get the top repositories for any topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "wound-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47300,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'},\n",
       " {'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45100,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'repository_name': 'TensorFlow-Examples',\n",
       "  'owner_username': 'aymericdamien',\n",
       "  'stars': 40300,\n",
       "  'repository_url': 'https://github.com/aymericdamien/TensorFlow-Examples'}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_page_ml = get_topic_page('machine-learning')\n",
    "top_repos_ml = get_top_repositories(topic_page_ml)\n",
    "top_repos_ml[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-field",
   "metadata": {},
   "source": [
    "Here are the top repositories for the keyword `data-analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "graduate-cassette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'scikit-learn',\n",
       "  'owner_username': 'scikit-learn',\n",
       "  'stars': 45100,\n",
       "  'repository_url': 'https://github.com/scikit-learn/scikit-learn'},\n",
       " {'repository_name': 'superset',\n",
       "  'owner_username': 'apache',\n",
       "  'stars': 36300,\n",
       "  'repository_url': 'https://github.com/apache/superset'},\n",
       " {'repository_name': 'pandas',\n",
       "  'owner_username': 'pandas-dev',\n",
       "  'stars': 29200,\n",
       "  'repository_url': 'https://github.com/pandas-dev/pandas'},\n",
       " {'repository_name': 'metabase',\n",
       "  'owner_username': 'metabase',\n",
       "  'stars': 24400,\n",
       "  'repository_url': 'https://github.com/metabase/metabase'},\n",
       " {'repository_name': 'streamlit',\n",
       "  'owner_username': 'streamlit',\n",
       "  'stars': 14000,\n",
       "  'repository_url': 'https://github.com/streamlit/streamlit'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_page_da = get_topic_page('data-analysis')\n",
    "top_repos_da = get_top_repositories(topic_page_da)\n",
    "top_repos_da[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-italian",
   "metadata": {},
   "source": [
    "Here are the top repositories for the keyword `python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "noble-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'system-design-primer',\n",
       "  'owner_username': 'donnemartin',\n",
       "  'stars': 125000,\n",
       "  'repository_url': 'https://github.com/donnemartin/system-design-primer'},\n",
       " {'repository_name': 'CS-Notes',\n",
       "  'owner_username': 'CyC2018',\n",
       "  'stars': 125000,\n",
       "  'repository_url': 'https://github.com/CyC2018/CS-Notes'},\n",
       " {'repository_name': 'Python',\n",
       "  'owner_username': 'TheAlgorithms',\n",
       "  'stars': 102000,\n",
       "  'repository_url': 'https://github.com/TheAlgorithms/Python'},\n",
       " {'repository_name': 'awesome-python',\n",
       "  'owner_username': 'vinta',\n",
       "  'stars': 95400,\n",
       "  'repository_url': 'https://github.com/vinta/awesome-python'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_repositories(get_topic_page('python'))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-person",
   "metadata": {},
   "source": [
    "Do you see the power of defining functions and using libraries? With just one line of code, we can scrape GitHub and find the top repositories for any topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-devon",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "needed-judge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n",
      "[jovian] Updating notebook \"aakashns/python-web-scraping-and-rest-api\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Uploading notebook..\u001b[0m\n",
      "[jovian] Capturing environment..\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/aakashns/python-web-scraping-and-rest-api\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/aakashns/python-web-scraping-and-rest-api'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-envelope",
   "metadata": {},
   "source": [
    "## Writing information to CSV files\n",
    "\n",
    "Let's create a helper function which takes a list of dictionaries and writes them to a CSV file.\n",
    "\n",
    "The input to our function will be a list of dictionary of the form:\n",
    "\n",
    "```\n",
    "[\n",
    "  {'key1': 'abc', 'key2': 'def', 'key3': 'ghi'},\n",
    "  {'key1': 'jkl', 'key2': 'mno', 'key3': 'pqr'},\n",
    "  {'key1': 'stu', 'key2': 'vwx', 'key3': 'yza'}\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "The function will create a file with a given name containing the following data:\n",
    "\n",
    "```\n",
    "key1,key2,key3\n",
    "abc,def,ghi\n",
    "jkl,mno,pqr\n",
    "stu,vwx,yza\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "indie-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(items, path):\n",
    "    # Open the file in write mode\n",
    "    with open(path, 'w') as f:\n",
    "        # Return if there's nothing to write\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        \n",
    "        # Write the headers in the first line\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        \n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-envelope",
   "metadata": {},
   "source": [
    "Let's write the data stored in `top_repos_ml` into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "worth-clearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_repos_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "conceptual-scotland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repository_name': 'tensorflow',\n",
       "  'owner_username': 'tensorflow',\n",
       "  'stars': 155000,\n",
       "  'repository_url': 'https://github.com/tensorflow/tensorflow'},\n",
       " {'repository_name': 'keras',\n",
       "  'owner_username': 'keras-team',\n",
       "  'stars': 51000,\n",
       "  'repository_url': 'https://github.com/keras-team/keras'},\n",
       " {'repository_name': 'pytorch',\n",
       "  'owner_username': 'pytorch',\n",
       "  'stars': 47300,\n",
       "  'repository_url': 'https://github.com/pytorch/pytorch'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_repos_ml[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "blond-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(top_repositories, 'machine-learning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-found",
   "metadata": {},
   "source": [
    "We can now read the file and inspect its contents. The contents of the file can also be inspected using the \"File > Open\" menu option within Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "backed-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repository_name,owner_username,stars,repository_url\n",
      "tensorflow,tensorflow,155000,https://github.com/tensorflow/tensorflow\n",
      "keras,keras-team,51000,https://github.com/keras-team/keras\n",
      "pytorch,pytorch,47300,https://github.com/pytorch/pytorch\n",
      "scikit-learn,scikit-learn,45100,https://github.com/scikit-learn/scikit-learn\n",
      "TensorFlow-Examples,aymericdamien,40300,https://github.com/aymericdamien/TensorFlow-Examples\n",
      "tesseract,tesseract-ocr,39400,https://github.com/tesseract-ocr/tesseract\n",
      "face_recognition,ageitgey,39200,https://github.com/ageitgey/face_recognition\n",
      "faceswap,deepfakes,34800,https://github.com/deepfakes/faceswap\n",
      "julia,JuliaLang,33100,https://github.com/JuliaLang/julia\n",
      "100-Days-Of-ML-Code,Avik-Jain,31800,https://github.com/Avik-Jain/100-Days-Of-ML-Code\n",
      "caffe,BVLC,31500,https://github.com/BVLC/caffe\n",
      "awesome-scalability,binhnguyennus,29700,https://github.com/binhnguyennus/awesome-scalability\n",
      "madewithml,GokuMohandas,25600,https://github.com/GokuMohandas/madewithml\n",
      "machine-learning-for-software-engineers,ZuzooVn,24900,https://github.com/ZuzooVn/machine-learning-for-software-engineers\n",
      "awesome-deep-learning-papers,terryum,22700,https://github.com/terryum/awesome-deep-learning-papers\n",
      "handson-ml,ageron,22600,https://github.com/ageron/handson-ml\n",
      "cs-video-courses,Developer-Y,22100,https://github.com/Developer-Y/cs-video-courses\n",
      "d2l-zh,d2l-ai,21900,https://github.com/d2l-ai/d2l-zh\n",
      "Coursera-ML-AndrewNg-Notes,fengdu78,21500,https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes\n",
      "data-science-ipython-notebooks,donnemartin,20900,https://github.com/donnemartin/data-science-ipython-notebooks\n",
      "xgboost,dmlc,20800,https://github.com/dmlc/xgboost\n",
      "fastai,fastai,20700,https://github.com/fastai/fastai\n",
      "openpose,CMU-Perceptual-Computing-Lab,20500,https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
      "spaCy,explosion,20100,https://github.com/explosion/spaCy\n",
      "ML-From-Scratch,eriklindernoren,19700,https://github.com/eriklindernoren/ML-From-Scratch\n",
      "NLP-progress,sebastianruder,18200,https://github.com/sebastianruder/NLP-progress\n",
      "homemade-machine-learning,trekhleb,17300,https://github.com/trekhleb/homemade-machine-learning\n",
      "CNTK,microsoft,17000,https://github.com/microsoft/CNTK\n",
      "DeepSpeech,mozilla,17000,https://github.com/mozilla/DeepSpeech\n",
      "awesome-deep-learning,ChristosChristofidis,16900,https://github.com/ChristosChristofidis/awesome-deep-learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('machine-learning.csv', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-occupation",
   "metadata": {},
   "source": [
    "Perfect! We've created a CSV containing the information about the top GitHub repositories for the topic `machine-learning`. We can now put together everything we've done so far to solve the original problem.\n",
    "\n",
    "> **QUESTION**: Write a Python function that creates a CSV file (comma-separated values) containing details about the 25 top GitHub repositories for any given topic. The top repositories for the topic `machine-learning` can be found on this page: [https://github.com/topics/machine-learning](https://github.com/topics/machine-learning). The output CSV should contain these details: repository name, owner's username, no. of stars, repository URL. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "higher-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "base_url = 'https://gitub.com'\n",
    "\n",
    "def scrape_topic_repositories(topic, path=None):\n",
    "    if path is None:\n",
    "        path = topic + '.csv'\n",
    "    topic_page_doc = get_topic_page(topic)\n",
    "    topic_repositories = get_top_repositories(topic_page_doc)\n",
    "    write_csv(topic_repositories, path)\n",
    "    print('Top repositories for topic \"{}\" written to file \"{}\"'.format(topic, path))\n",
    "    return path\n",
    "\n",
    "def get_top_repositories(doc):\n",
    "    article_tags = doc.find_all('article', class_='border rounded color-shadow-small color-bg-secondary my-4')\n",
    "    topic_repos = [parse_repository(tag) for tag in article_tags]\n",
    "    return topic_repos\n",
    "\n",
    "def get_topic_page(topic):\n",
    "    topic_repos_url = 'https://github.com/topics/' + topic\n",
    "    response = requests.get(topic_repos_url)\n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to fetch web page ' + topic_repos_url)\n",
    "    return BeautifulSoup(response.text)    \n",
    "\n",
    "def parse_repository(article_tag):\n",
    "    a_tags = article_tag.h1.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href'].strip()\n",
    "    stars_tag = article_tag.find('a', class_='social-count float-none')\n",
    "    star_count = parse_star_count(stars_tag.text.strip())\n",
    "    return {'repository_name': repo_name, 'owner_username': username, 'stars': star_count, 'repository_url': repo_url}\n",
    "\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    return int(float(stars_str[:-1]) * 1000) if stars_str[-1] == 'k' else int(stars_str)\n",
    "\n",
    "def write_csv(items, path):\n",
    "    with open(path, 'w') as f:\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-rabbit",
   "metadata": {},
   "source": [
    "The entire code of this problem is only about 50 lines long. Isn't that neat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "prostate-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top repositories for topic \"machine-learning\" written to file \"machine-learning.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'machine-learning.csv'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topic_repositories('machine-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-senegal",
   "metadata": {},
   "source": [
    "Now that we have a CSV file, we can use the `pandas` library to view its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "smart-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "limited-hollow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>owner_username</th>\n",
       "      <th>stars</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>155000</td>\n",
       "      <td>https://gitub.com/tensorflow/tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keras</td>\n",
       "      <td>keras-team</td>\n",
       "      <td>51000</td>\n",
       "      <td>https://gitub.com/keras-team/keras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>47300</td>\n",
       "      <td>https://gitub.com/pytorch/pytorch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>45100</td>\n",
       "      <td>https://gitub.com/scikit-learn/scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TensorFlow-Examples</td>\n",
       "      <td>aymericdamien</td>\n",
       "      <td>40300</td>\n",
       "      <td>https://gitub.com/aymericdamien/TensorFlow-Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tesseract</td>\n",
       "      <td>tesseract-ocr</td>\n",
       "      <td>39400</td>\n",
       "      <td>https://gitub.com/tesseract-ocr/tesseract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>face_recognition</td>\n",
       "      <td>ageitgey</td>\n",
       "      <td>39200</td>\n",
       "      <td>https://gitub.com/ageitgey/face_recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>faceswap</td>\n",
       "      <td>deepfakes</td>\n",
       "      <td>34800</td>\n",
       "      <td>https://gitub.com/deepfakes/faceswap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>julia</td>\n",
       "      <td>JuliaLang</td>\n",
       "      <td>33100</td>\n",
       "      <td>https://gitub.com/JuliaLang/julia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100-Days-Of-ML-Code</td>\n",
       "      <td>Avik-Jain</td>\n",
       "      <td>31800</td>\n",
       "      <td>https://gitub.com/Avik-Jain/100-Days-Of-ML-Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>caffe</td>\n",
       "      <td>BVLC</td>\n",
       "      <td>31500</td>\n",
       "      <td>https://gitub.com/BVLC/caffe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>awesome-scalability</td>\n",
       "      <td>binhnguyennus</td>\n",
       "      <td>29700</td>\n",
       "      <td>https://gitub.com/binhnguyennus/awesome-scalab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>madewithml</td>\n",
       "      <td>GokuMohandas</td>\n",
       "      <td>25600</td>\n",
       "      <td>https://gitub.com/GokuMohandas/madewithml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>machine-learning-for-software-engineers</td>\n",
       "      <td>ZuzooVn</td>\n",
       "      <td>24900</td>\n",
       "      <td>https://gitub.com/ZuzooVn/machine-learning-for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>awesome-deep-learning-papers</td>\n",
       "      <td>terryum</td>\n",
       "      <td>22700</td>\n",
       "      <td>https://gitub.com/terryum/awesome-deep-learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>handson-ml</td>\n",
       "      <td>ageron</td>\n",
       "      <td>22600</td>\n",
       "      <td>https://gitub.com/ageron/handson-ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cs-video-courses</td>\n",
       "      <td>Developer-Y</td>\n",
       "      <td>22100</td>\n",
       "      <td>https://gitub.com/Developer-Y/cs-video-courses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>d2l-zh</td>\n",
       "      <td>d2l-ai</td>\n",
       "      <td>21900</td>\n",
       "      <td>https://gitub.com/d2l-ai/d2l-zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Coursera-ML-AndrewNg-Notes</td>\n",
       "      <td>fengdu78</td>\n",
       "      <td>21500</td>\n",
       "      <td>https://gitub.com/fengdu78/Coursera-ML-AndrewN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data-science-ipython-notebooks</td>\n",
       "      <td>donnemartin</td>\n",
       "      <td>20900</td>\n",
       "      <td>https://gitub.com/donnemartin/data-science-ipy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>dmlc</td>\n",
       "      <td>20800</td>\n",
       "      <td>https://gitub.com/dmlc/xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fastai</td>\n",
       "      <td>fastai</td>\n",
       "      <td>20700</td>\n",
       "      <td>https://gitub.com/fastai/fastai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>openpose</td>\n",
       "      <td>CMU-Perceptual-Computing-Lab</td>\n",
       "      <td>20500</td>\n",
       "      <td>https://gitub.com/CMU-Perceptual-Computing-Lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spaCy</td>\n",
       "      <td>explosion</td>\n",
       "      <td>20100</td>\n",
       "      <td>https://gitub.com/explosion/spaCy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ML-From-Scratch</td>\n",
       "      <td>eriklindernoren</td>\n",
       "      <td>19700</td>\n",
       "      <td>https://gitub.com/eriklindernoren/ML-From-Scratch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NLP-progress</td>\n",
       "      <td>sebastianruder</td>\n",
       "      <td>18200</td>\n",
       "      <td>https://gitub.com/sebastianruder/NLP-progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>homemade-machine-learning</td>\n",
       "      <td>trekhleb</td>\n",
       "      <td>17300</td>\n",
       "      <td>https://gitub.com/trekhleb/homemade-machine-le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CNTK</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>17000</td>\n",
       "      <td>https://gitub.com/microsoft/CNTK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DeepSpeech</td>\n",
       "      <td>mozilla</td>\n",
       "      <td>17000</td>\n",
       "      <td>https://gitub.com/mozilla/DeepSpeech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>awesome-deep-learning</td>\n",
       "      <td>ChristosChristofidis</td>\n",
       "      <td>16900</td>\n",
       "      <td>https://gitub.com/ChristosChristofidis/awesome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            repository_name                owner_username  \\\n",
       "0                                tensorflow                    tensorflow   \n",
       "1                                     keras                    keras-team   \n",
       "2                                   pytorch                       pytorch   \n",
       "3                              scikit-learn                  scikit-learn   \n",
       "4                       TensorFlow-Examples                 aymericdamien   \n",
       "5                                 tesseract                 tesseract-ocr   \n",
       "6                          face_recognition                      ageitgey   \n",
       "7                                  faceswap                     deepfakes   \n",
       "8                                     julia                     JuliaLang   \n",
       "9                       100-Days-Of-ML-Code                     Avik-Jain   \n",
       "10                                    caffe                          BVLC   \n",
       "11                      awesome-scalability                 binhnguyennus   \n",
       "12                               madewithml                  GokuMohandas   \n",
       "13  machine-learning-for-software-engineers                       ZuzooVn   \n",
       "14             awesome-deep-learning-papers                       terryum   \n",
       "15                               handson-ml                        ageron   \n",
       "16                         cs-video-courses                   Developer-Y   \n",
       "17                                   d2l-zh                        d2l-ai   \n",
       "18               Coursera-ML-AndrewNg-Notes                      fengdu78   \n",
       "19           data-science-ipython-notebooks                   donnemartin   \n",
       "20                                  xgboost                          dmlc   \n",
       "21                                   fastai                        fastai   \n",
       "22                                 openpose  CMU-Perceptual-Computing-Lab   \n",
       "23                                    spaCy                     explosion   \n",
       "24                          ML-From-Scratch               eriklindernoren   \n",
       "25                             NLP-progress                sebastianruder   \n",
       "26                homemade-machine-learning                      trekhleb   \n",
       "27                                     CNTK                     microsoft   \n",
       "28                               DeepSpeech                       mozilla   \n",
       "29                    awesome-deep-learning          ChristosChristofidis   \n",
       "\n",
       "     stars                                     repository_url  \n",
       "0   155000            https://gitub.com/tensorflow/tensorflow  \n",
       "1    51000                 https://gitub.com/keras-team/keras  \n",
       "2    47300                  https://gitub.com/pytorch/pytorch  \n",
       "3    45100        https://gitub.com/scikit-learn/scikit-learn  \n",
       "4    40300  https://gitub.com/aymericdamien/TensorFlow-Exa...  \n",
       "5    39400          https://gitub.com/tesseract-ocr/tesseract  \n",
       "6    39200        https://gitub.com/ageitgey/face_recognition  \n",
       "7    34800               https://gitub.com/deepfakes/faceswap  \n",
       "8    33100                  https://gitub.com/JuliaLang/julia  \n",
       "9    31800    https://gitub.com/Avik-Jain/100-Days-Of-ML-Code  \n",
       "10   31500                       https://gitub.com/BVLC/caffe  \n",
       "11   29700  https://gitub.com/binhnguyennus/awesome-scalab...  \n",
       "12   25600          https://gitub.com/GokuMohandas/madewithml  \n",
       "13   24900  https://gitub.com/ZuzooVn/machine-learning-for...  \n",
       "14   22700  https://gitub.com/terryum/awesome-deep-learnin...  \n",
       "15   22600                https://gitub.com/ageron/handson-ml  \n",
       "16   22100     https://gitub.com/Developer-Y/cs-video-courses  \n",
       "17   21900                    https://gitub.com/d2l-ai/d2l-zh  \n",
       "18   21500  https://gitub.com/fengdu78/Coursera-ML-AndrewN...  \n",
       "19   20900  https://gitub.com/donnemartin/data-science-ipy...  \n",
       "20   20800                     https://gitub.com/dmlc/xgboost  \n",
       "21   20700                    https://gitub.com/fastai/fastai  \n",
       "22   20500  https://gitub.com/CMU-Perceptual-Computing-Lab...  \n",
       "23   20100                  https://gitub.com/explosion/spaCy  \n",
       "24   19700  https://gitub.com/eriklindernoren/ML-From-Scratch  \n",
       "25   18200      https://gitub.com/sebastianruder/NLP-progress  \n",
       "26   17300  https://gitub.com/trekhleb/homemade-machine-le...  \n",
       "27   17000                   https://gitub.com/microsoft/CNTK  \n",
       "28   17000               https://gitub.com/mozilla/DeepSpeech  \n",
       "29   16900  https://gitub.com/ChristosChristofidis/awesome...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('machine-learning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "challenging-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top repositories for topic \"data-analysis\" written to file \"data-analysis.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data-analysis.csv'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topic_repositories('data-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "interested-batman",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>owner_username</th>\n",
       "      <th>stars</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>45100</td>\n",
       "      <td>https://gitub.com/scikit-learn/scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>superset</td>\n",
       "      <td>apache</td>\n",
       "      <td>36300</td>\n",
       "      <td>https://gitub.com/apache/superset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pandas</td>\n",
       "      <td>pandas-dev</td>\n",
       "      <td>29200</td>\n",
       "      <td>https://gitub.com/pandas-dev/pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metabase</td>\n",
       "      <td>metabase</td>\n",
       "      <td>24400</td>\n",
       "      <td>https://gitub.com/metabase/metabase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>streamlit</td>\n",
       "      <td>streamlit</td>\n",
       "      <td>14000</td>\n",
       "      <td>https://gitub.com/streamlit/streamlit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goaccess</td>\n",
       "      <td>allinurl</td>\n",
       "      <td>13000</td>\n",
       "      <td>https://gitub.com/allinurl/goaccess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CyberChef</td>\n",
       "      <td>gchq</td>\n",
       "      <td>11600</td>\n",
       "      <td>https://gitub.com/gchq/CyberChef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AI-Expert-Roadmap</td>\n",
       "      <td>AMAI-GmbH</td>\n",
       "      <td>9700</td>\n",
       "      <td>https://gitub.com/AMAI-GmbH/AI-Expert-Roadmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OpenRefine</td>\n",
       "      <td>OpenRefine</td>\n",
       "      <td>8000</td>\n",
       "      <td>https://gitub.com/OpenRefine/OpenRefine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mlcourse.ai</td>\n",
       "      <td>Yorko</td>\n",
       "      <td>7500</td>\n",
       "      <td>https://gitub.com/Yorko/mlcourse.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pandas-profiling</td>\n",
       "      <td>pandas-profiling</td>\n",
       "      <td>7000</td>\n",
       "      <td>https://gitub.com/pandas-profiling/pandas-prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>statsmodels</td>\n",
       "      <td>statsmodels</td>\n",
       "      <td>6200</td>\n",
       "      <td>https://gitub.com/statsmodels/statsmodels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pandas_exercises</td>\n",
       "      <td>guipsamora</td>\n",
       "      <td>6000</td>\n",
       "      <td>https://gitub.com/guipsamora/pandas_exercises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>imbalanced-learn</td>\n",
       "      <td>scikit-learn-contrib</td>\n",
       "      <td>5100</td>\n",
       "      <td>https://gitub.com/scikit-learn-contrib/imbalan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>alluxio</td>\n",
       "      <td>Alluxio</td>\n",
       "      <td>5000</td>\n",
       "      <td>https://gitub.com/Alluxio/alluxio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data-Analysis-and-Machine-Learning-Projects</td>\n",
       "      <td>rhiever</td>\n",
       "      <td>5000</td>\n",
       "      <td>https://gitub.com/rhiever/Data-Analysis-and-Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pachyderm</td>\n",
       "      <td>pachyderm</td>\n",
       "      <td>5000</td>\n",
       "      <td>https://gitub.com/pachyderm/pachyderm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gonum</td>\n",
       "      <td>gonum</td>\n",
       "      <td>4700</td>\n",
       "      <td>https://gitub.com/gonum/gonum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>knowledge-repo</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>4700</td>\n",
       "      <td>https://gitub.com/airbnb/knowledge-repo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weibospider</td>\n",
       "      <td>SpiderClub</td>\n",
       "      <td>4600</td>\n",
       "      <td>https://gitub.com/SpiderClub/weibospider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gop</td>\n",
       "      <td>goplus</td>\n",
       "      <td>4500</td>\n",
       "      <td>https://gitub.com/goplus/gop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>awesome-R</td>\n",
       "      <td>qinwf</td>\n",
       "      <td>4500</td>\n",
       "      <td>https://gitub.com/qinwf/awesome-R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pyod</td>\n",
       "      <td>yzhao062</td>\n",
       "      <td>4300</td>\n",
       "      <td>https://gitub.com/yzhao062/pyod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pydata-notebook</td>\n",
       "      <td>BrambleXu</td>\n",
       "      <td>4100</td>\n",
       "      <td>https://gitub.com/BrambleXu/pydata-notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sqlpad</td>\n",
       "      <td>sqlpad</td>\n",
       "      <td>3700</td>\n",
       "      <td>https://gitub.com/sqlpad/sqlpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>akshare</td>\n",
       "      <td>jindaxiang</td>\n",
       "      <td>3300</td>\n",
       "      <td>https://gitub.com/jindaxiang/akshare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ai-Learn</td>\n",
       "      <td>tangyudi</td>\n",
       "      <td>3200</td>\n",
       "      <td>https://gitub.com/tangyudi/Ai-Learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>xlearn</td>\n",
       "      <td>aksnzhy</td>\n",
       "      <td>2900</td>\n",
       "      <td>https://gitub.com/aksnzhy/xlearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>missingno</td>\n",
       "      <td>ResidentMario</td>\n",
       "      <td>2700</td>\n",
       "      <td>https://gitub.com/ResidentMario/missingno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>plotnine</td>\n",
       "      <td>has2k1</td>\n",
       "      <td>2600</td>\n",
       "      <td>https://gitub.com/has2k1/plotnine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                repository_name        owner_username  stars  \\\n",
       "0                                  scikit-learn          scikit-learn  45100   \n",
       "1                                      superset                apache  36300   \n",
       "2                                        pandas            pandas-dev  29200   \n",
       "3                                      metabase              metabase  24400   \n",
       "4                                     streamlit             streamlit  14000   \n",
       "5                                      goaccess              allinurl  13000   \n",
       "6                                     CyberChef                  gchq  11600   \n",
       "7                             AI-Expert-Roadmap             AMAI-GmbH   9700   \n",
       "8                                    OpenRefine            OpenRefine   8000   \n",
       "9                                   mlcourse.ai                 Yorko   7500   \n",
       "10                             pandas-profiling      pandas-profiling   7000   \n",
       "11                                  statsmodels           statsmodels   6200   \n",
       "12                             pandas_exercises            guipsamora   6000   \n",
       "13                             imbalanced-learn  scikit-learn-contrib   5100   \n",
       "14                                      alluxio               Alluxio   5000   \n",
       "15  Data-Analysis-and-Machine-Learning-Projects               rhiever   5000   \n",
       "16                                    pachyderm             pachyderm   5000   \n",
       "17                                        gonum                 gonum   4700   \n",
       "18                               knowledge-repo                airbnb   4700   \n",
       "19                                  weibospider            SpiderClub   4600   \n",
       "20                                          gop                goplus   4500   \n",
       "21                                    awesome-R                 qinwf   4500   \n",
       "22                                         pyod              yzhao062   4300   \n",
       "23                              pydata-notebook             BrambleXu   4100   \n",
       "24                                       sqlpad                sqlpad   3700   \n",
       "25                                      akshare            jindaxiang   3300   \n",
       "26                                     Ai-Learn              tangyudi   3200   \n",
       "27                                       xlearn               aksnzhy   2900   \n",
       "28                                    missingno         ResidentMario   2700   \n",
       "29                                     plotnine                has2k1   2600   \n",
       "\n",
       "                                       repository_url  \n",
       "0         https://gitub.com/scikit-learn/scikit-learn  \n",
       "1                   https://gitub.com/apache/superset  \n",
       "2                 https://gitub.com/pandas-dev/pandas  \n",
       "3                 https://gitub.com/metabase/metabase  \n",
       "4               https://gitub.com/streamlit/streamlit  \n",
       "5                 https://gitub.com/allinurl/goaccess  \n",
       "6                    https://gitub.com/gchq/CyberChef  \n",
       "7       https://gitub.com/AMAI-GmbH/AI-Expert-Roadmap  \n",
       "8             https://gitub.com/OpenRefine/OpenRefine  \n",
       "9                 https://gitub.com/Yorko/mlcourse.ai  \n",
       "10  https://gitub.com/pandas-profiling/pandas-prof...  \n",
       "11          https://gitub.com/statsmodels/statsmodels  \n",
       "12      https://gitub.com/guipsamora/pandas_exercises  \n",
       "13  https://gitub.com/scikit-learn-contrib/imbalan...  \n",
       "14                  https://gitub.com/Alluxio/alluxio  \n",
       "15  https://gitub.com/rhiever/Data-Analysis-and-Ma...  \n",
       "16              https://gitub.com/pachyderm/pachyderm  \n",
       "17                      https://gitub.com/gonum/gonum  \n",
       "18            https://gitub.com/airbnb/knowledge-repo  \n",
       "19           https://gitub.com/SpiderClub/weibospider  \n",
       "20                       https://gitub.com/goplus/gop  \n",
       "21                  https://gitub.com/qinwf/awesome-R  \n",
       "22                    https://gitub.com/yzhao062/pyod  \n",
       "23        https://gitub.com/BrambleXu/pydata-notebook  \n",
       "24                    https://gitub.com/sqlpad/sqlpad  \n",
       "25               https://gitub.com/jindaxiang/akshare  \n",
       "26                https://gitub.com/tangyudi/Ai-Learn  \n",
       "27                   https://gitub.com/aksnzhy/xlearn  \n",
       "28          https://gitub.com/ResidentMario/missingno  \n",
       "29                  https://gitub.com/has2k1/plotnine  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data-analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "mature-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top repositories for topic \"python\" written to file \"python.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'python.csv'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topic_repositories('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "backed-stationery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>owner_username</th>\n",
       "      <th>stars</th>\n",
       "      <th>repository_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>155000</td>\n",
       "      <td>https://gitub.com/tensorflow/tensorflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>system-design-primer</td>\n",
       "      <td>donnemartin</td>\n",
       "      <td>125000</td>\n",
       "      <td>https://gitub.com/donnemartin/system-design-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CS-Notes</td>\n",
       "      <td>CyC2018</td>\n",
       "      <td>125000</td>\n",
       "      <td>https://gitub.com/CyC2018/CS-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>TheAlgorithms</td>\n",
       "      <td>102000</td>\n",
       "      <td>https://gitub.com/TheAlgorithms/Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome-python</td>\n",
       "      <td>vinta</td>\n",
       "      <td>95400</td>\n",
       "      <td>https://gitub.com/vinta/awesome-python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>free-programming-books-zh_CN</td>\n",
       "      <td>justjavac</td>\n",
       "      <td>78200</td>\n",
       "      <td>https://gitub.com/justjavac/free-programming-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thefuck</td>\n",
       "      <td>nvbn</td>\n",
       "      <td>59600</td>\n",
       "      <td>https://gitub.com/nvbn/thefuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>django</td>\n",
       "      <td>django</td>\n",
       "      <td>56500</td>\n",
       "      <td>https://gitub.com/django/django</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flask</td>\n",
       "      <td>pallets</td>\n",
       "      <td>54400</td>\n",
       "      <td>https://gitub.com/pallets/flask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>keras</td>\n",
       "      <td>keras-team</td>\n",
       "      <td>51000</td>\n",
       "      <td>https://gitub.com/keras-team/keras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>httpie</td>\n",
       "      <td>httpie</td>\n",
       "      <td>50300</td>\n",
       "      <td>https://gitub.com/httpie/httpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ansible</td>\n",
       "      <td>ansible</td>\n",
       "      <td>47600</td>\n",
       "      <td>https://gitub.com/ansible/ansible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>47300</td>\n",
       "      <td>https://gitub.com/pytorch/pytorch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>project-based-learning</td>\n",
       "      <td>tuvtran</td>\n",
       "      <td>46900</td>\n",
       "      <td>https://gitub.com/tuvtran/project-based-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>45100</td>\n",
       "      <td>https://gitub.com/scikit-learn/scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>requests</td>\n",
       "      <td>psf</td>\n",
       "      <td>44900</td>\n",
       "      <td>https://gitub.com/psf/requests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>core</td>\n",
       "      <td>home-assistant</td>\n",
       "      <td>41200</td>\n",
       "      <td>https://gitub.com/home-assistant/core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>leetcode</td>\n",
       "      <td>azl397985856</td>\n",
       "      <td>41000</td>\n",
       "      <td>https://gitub.com/azl397985856/leetcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TensorFlow-Examples</td>\n",
       "      <td>aymericdamien</td>\n",
       "      <td>40300</td>\n",
       "      <td>https://gitub.com/aymericdamien/TensorFlow-Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>scrapy</td>\n",
       "      <td>scrapy</td>\n",
       "      <td>40200</td>\n",
       "      <td>https://gitub.com/scrapy/scrapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HelloGitHub</td>\n",
       "      <td>521xueweihan</td>\n",
       "      <td>39500</td>\n",
       "      <td>https://gitub.com/521xueweihan/HelloGitHub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>face_recognition</td>\n",
       "      <td>ageitgey</td>\n",
       "      <td>39200</td>\n",
       "      <td>https://gitub.com/ageitgey/face_recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>superset</td>\n",
       "      <td>apache</td>\n",
       "      <td>36300</td>\n",
       "      <td>https://gitub.com/apache/superset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>manim</td>\n",
       "      <td>3b1b</td>\n",
       "      <td>32400</td>\n",
       "      <td>https://gitub.com/3b1b/manim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100-Days-Of-ML-Code</td>\n",
       "      <td>Avik-Jain</td>\n",
       "      <td>31800</td>\n",
       "      <td>https://gitub.com/Avik-Jain/100-Days-Of-ML-Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AiLearning</td>\n",
       "      <td>apachecn</td>\n",
       "      <td>29300</td>\n",
       "      <td>https://gitub.com/apachecn/AiLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spark</td>\n",
       "      <td>apache</td>\n",
       "      <td>29200</td>\n",
       "      <td>https://gitub.com/apache/spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pandas</td>\n",
       "      <td>pandas-dev</td>\n",
       "      <td>29200</td>\n",
       "      <td>https://gitub.com/pandas-dev/pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fastapi</td>\n",
       "      <td>tiangolo</td>\n",
       "      <td>29200</td>\n",
       "      <td>https://gitub.com/tiangolo/fastapi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PythonDataScienceHandbook</td>\n",
       "      <td>jakevdp</td>\n",
       "      <td>28600</td>\n",
       "      <td>https://gitub.com/jakevdp/PythonDataScienceHan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 repository_name  owner_username   stars  \\\n",
       "0                     tensorflow      tensorflow  155000   \n",
       "1           system-design-primer     donnemartin  125000   \n",
       "2                       CS-Notes         CyC2018  125000   \n",
       "3                         Python   TheAlgorithms  102000   \n",
       "4                 awesome-python           vinta   95400   \n",
       "5   free-programming-books-zh_CN       justjavac   78200   \n",
       "6                        thefuck            nvbn   59600   \n",
       "7                         django          django   56500   \n",
       "8                          flask         pallets   54400   \n",
       "9                          keras      keras-team   51000   \n",
       "10                        httpie          httpie   50300   \n",
       "11                       ansible         ansible   47600   \n",
       "12                       pytorch         pytorch   47300   \n",
       "13        project-based-learning         tuvtran   46900   \n",
       "14                  scikit-learn    scikit-learn   45100   \n",
       "15                      requests             psf   44900   \n",
       "16                          core  home-assistant   41200   \n",
       "17                      leetcode    azl397985856   41000   \n",
       "18           TensorFlow-Examples   aymericdamien   40300   \n",
       "19                        scrapy          scrapy   40200   \n",
       "20                   HelloGitHub    521xueweihan   39500   \n",
       "21              face_recognition        ageitgey   39200   \n",
       "22                      superset          apache   36300   \n",
       "23                         manim            3b1b   32400   \n",
       "24           100-Days-Of-ML-Code       Avik-Jain   31800   \n",
       "25                    AiLearning        apachecn   29300   \n",
       "26                         spark          apache   29200   \n",
       "27                        pandas      pandas-dev   29200   \n",
       "28                       fastapi        tiangolo   29200   \n",
       "29     PythonDataScienceHandbook         jakevdp   28600   \n",
       "\n",
       "                                       repository_url  \n",
       "0             https://gitub.com/tensorflow/tensorflow  \n",
       "1   https://gitub.com/donnemartin/system-design-pr...  \n",
       "2                  https://gitub.com/CyC2018/CS-Notes  \n",
       "3              https://gitub.com/TheAlgorithms/Python  \n",
       "4              https://gitub.com/vinta/awesome-python  \n",
       "5   https://gitub.com/justjavac/free-programming-b...  \n",
       "6                      https://gitub.com/nvbn/thefuck  \n",
       "7                     https://gitub.com/django/django  \n",
       "8                     https://gitub.com/pallets/flask  \n",
       "9                  https://gitub.com/keras-team/keras  \n",
       "10                    https://gitub.com/httpie/httpie  \n",
       "11                  https://gitub.com/ansible/ansible  \n",
       "12                  https://gitub.com/pytorch/pytorch  \n",
       "13   https://gitub.com/tuvtran/project-based-learning  \n",
       "14        https://gitub.com/scikit-learn/scikit-learn  \n",
       "15                     https://gitub.com/psf/requests  \n",
       "16              https://gitub.com/home-assistant/core  \n",
       "17            https://gitub.com/azl397985856/leetcode  \n",
       "18  https://gitub.com/aymericdamien/TensorFlow-Exa...  \n",
       "19                    https://gitub.com/scrapy/scrapy  \n",
       "20         https://gitub.com/521xueweihan/HelloGitHub  \n",
       "21        https://gitub.com/ageitgey/face_recognition  \n",
       "22                  https://gitub.com/apache/superset  \n",
       "23                       https://gitub.com/3b1b/manim  \n",
       "24    https://gitub.com/Avik-Jain/100-Days-Of-ML-Code  \n",
       "25              https://gitub.com/apachecn/AiLearning  \n",
       "26                     https://gitub.com/apache/spark  \n",
       "27                https://gitub.com/pandas-dev/pandas  \n",
       "28                 https://gitub.com/tiangolo/fastapi  \n",
       "29  https://gitub.com/jakevdp/PythonDataScienceHan...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('python.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-amino",
   "metadata": {},
   "source": [
    "Of course, we can go even further and write a function that scrapes\n",
    "\n",
    "> **EXERCISE**: Write a function `scrape_topics` which takes a list of topics and creates CSV files containing top repositories for all the topics. Test it out using the empty cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-instrument",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-heather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-hurricane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "foster-plastic",
   "metadata": {},
   "source": [
    "Let's save our work before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-perry",
   "metadata": {},
   "source": [
    "## Using a REST API to retrieve data as JSON\n",
    "\n",
    "Not all URLs point to . In fact, many websites offer a REST API to access data in the JSON format.\n",
    "\n",
    "* Github\n",
    "* Twitter\n",
    "* Facebook\n",
    "* LinkedIn\n",
    "* Slack\n",
    "\n",
    "In case you're wondering what the acronyms mean, here are the expansions (you needn't remember or care about any of them):\n",
    "- REST - \"Represetational State Transfer\"\n",
    "- API - \"Application Programming Interface\"\n",
    "- JSON - \"JavaScript Object Notation\"\n",
    "- URL - \"Universal Resource Locator\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> **QUESTION**: For the above problem, augment the repository information with some more details: description, watcher count, fork count, open issues count, created at time and updated at time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-atlantic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-occupation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-designer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-london",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-lotus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-mirror",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "backed-large",
   "metadata": {},
   "source": [
    "## Crawling Websites by Parsing Links on a Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-guard",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Get the top 100 repositories for the all the featured topics on GitHub. Second page URL: https://github.com/topics/machine-learning?page=2 and topic pages are https://github.com/topics/?page=8. Create a different CSV file for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-money",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-rover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-customer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-nylon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-jordan",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-rubber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-courage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-youth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "reflected-characteristic",
   "metadata": {},
   "source": [
    "## Summary and Further Reading\n",
    "\n",
    "\n",
    "Some things to keep in mind with web scraping:\n",
    "\n",
    "* Most websites disallow web scraping for commercial purposes\n",
    "* Use web scraping only for learning and research purpose\n",
    "* Review the terms and conditions of a website before scraping\n",
    "* Prefer removing personally identifiable information before publishing a dataset online\n",
    "* Use official REST APIs wherever available, with proper keys\n",
    "* Scraping data that you see after logging in is harder to do (it requires special cookies and headers)\n",
    "* Websites may change their structure, in which case your code may no longer work\n",
    "\n",
    "Some\n",
    "\n",
    "\n",
    "Here are some more examples of scraping:\n",
    "\n",
    "* https://medium.com/@msalmon00/web-scraping-job-postings-from-indeed-96bd588dcb4b\n",
    "* https://medium.com/the-innovation/scraping-medium-with-python-beautiful-soup-3314f898bbf5\n",
    "* https://medium.com/brainstation23/how-to-become-a-pro-with-scraping-youtube-videos-in-3-minutes-a6ac56021961\n",
    "* https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\n",
    "* https://www.freecodecamp.org/news/scraping-wikipedia-articles-with-python/\n",
    "* https://towardsdatascience.com/web-scraping-yahoo-finance-477fe3daa852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-thunder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-acoustic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
